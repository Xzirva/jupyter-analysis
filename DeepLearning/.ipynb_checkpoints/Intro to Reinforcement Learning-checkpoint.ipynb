{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/Users/yoann/anaconda2/envs/python-3-tensorflow/lib/python36.zip',\n",
       " '/Users/yoann/anaconda2/envs/python-3-tensorflow/lib/python3.6',\n",
       " '/Users/yoann/anaconda2/envs/python-3-tensorflow/lib/python3.6/lib-dynload',\n",
       " '/Users/yoann/anaconda2/envs/python-3-tensorflow/lib/python3.6/site-packages',\n",
       " '/Users/yoann/anaconda2/envs/python-3-tensorflow/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/Users/yoann/.ipython']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sum_reward_agent(env, num_episodes=500):\n",
    "    # this is the table that will hold our summated rewards for\n",
    "    # each action in each state\n",
    "    r_table = np.zeros((5, 2))\n",
    "    for g in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(r_table[s, :]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with highest cummulative reward\n",
    "                a = np.argmax(r_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            r_table[s, a] += r\n",
    "            s = new_s\n",
    "    return r_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0., 640010.],\n",
       "       [     0., 127348.],\n",
       "       [     0.,  25700.],\n",
       "       [     0.,   5282.],\n",
       "       [     0.,   3030.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('NChain-v0')\n",
    "naive_sum_reward_agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q(s,a)=Q(s,a)+α(r+γ max(a′)Q(s′,a′)–Q(s,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    lr = 0.8\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(q_table[s,:]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with largest q value in state s\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31.09398826,  0.        ],\n",
       "       [ 0.        , 33.70117986],\n",
       "       [30.18151409,  0.        ],\n",
       "       [32.86145085,  0.        ],\n",
       "       [42.26284846,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    lr = 0.8\n",
    "    decay_factor = 0.999\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action with highest cummulative reward\n",
    "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            # pdb.set_trace()\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.67629833, 43.50590356],\n",
       "       [44.67459655, 35.46077865],\n",
       "       [46.56422311, 45.31904025],\n",
       "       [45.60190522, 47.67942429],\n",
       "       [47.91592189, 39.33203633]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps_greedy_q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methods(env, num_iterations=100):\n",
    "    winner = np.zeros((3,))\n",
    "    for g in range(num_iterations):\n",
    "        m0_table = naive_sum_reward_agent(env, 500)\n",
    "        m1_table = q_learning_with_table(env, 500)\n",
    "        m2_table = eps_greedy_q_learning_with_table(env, 500)\n",
    "        m0 = run_game(m0_table, env)\n",
    "        m1 = run_game(m1_table, env)\n",
    "        m2 = run_game(m2_table, env)\n",
    "        w = np.argmax(np.array([m0, m1, m2]))\n",
    "        winner[w] += 1\n",
    "        print(\"Game {} of {}\".format(g + 1, num_iterations))\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(table, env):\n",
    "    s = env.reset()\n",
    "    tot_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(table[s, :])\n",
    "        s, r, done, _ = env.step(a)\n",
    "        tot_reward += r\n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 of 100\n",
      "Game 2 of 100\n",
      "Game 3 of 100\n",
      "Game 4 of 100\n",
      "Game 5 of 100\n",
      "Game 6 of 100\n",
      "Game 7 of 100\n",
      "Game 8 of 100\n",
      "Game 9 of 100\n",
      "Game 10 of 100\n",
      "Game 11 of 100\n",
      "Game 12 of 100\n",
      "Game 13 of 100\n",
      "Game 14 of 100\n",
      "Game 15 of 100\n",
      "Game 16 of 100\n",
      "Game 17 of 100\n",
      "Game 18 of 100\n",
      "Game 19 of 100\n",
      "Game 20 of 100\n",
      "Game 21 of 100\n",
      "Game 22 of 100\n",
      "Game 23 of 100\n",
      "Game 24 of 100\n",
      "Game 25 of 100\n",
      "Game 26 of 100\n",
      "Game 27 of 100\n",
      "Game 28 of 100\n",
      "Game 29 of 100\n",
      "Game 30 of 100\n",
      "Game 31 of 100\n",
      "Game 32 of 100\n",
      "Game 33 of 100\n",
      "Game 34 of 100\n",
      "Game 35 of 100\n",
      "Game 36 of 100\n",
      "Game 37 of 100\n",
      "Game 38 of 100\n",
      "Game 39 of 100\n",
      "Game 40 of 100\n",
      "Game 41 of 100\n",
      "Game 42 of 100\n",
      "Game 43 of 100\n",
      "Game 44 of 100\n",
      "Game 45 of 100\n",
      "Game 46 of 100\n",
      "Game 47 of 100\n",
      "Game 48 of 100\n",
      "Game 49 of 100\n",
      "Game 50 of 100\n",
      "Game 51 of 100\n",
      "Game 52 of 100\n",
      "Game 53 of 100\n",
      "Game 54 of 100\n",
      "Game 55 of 100\n",
      "Game 56 of 100\n",
      "Game 57 of 100\n",
      "Game 58 of 100\n",
      "Game 59 of 100\n",
      "Game 60 of 100\n",
      "Game 61 of 100\n",
      "Game 62 of 100\n",
      "Game 63 of 100\n",
      "Game 64 of 100\n",
      "Game 65 of 100\n",
      "Game 66 of 100\n",
      "Game 67 of 100\n",
      "Game 68 of 100\n",
      "Game 69 of 100\n",
      "Game 70 of 100\n",
      "Game 71 of 100\n",
      "Game 72 of 100\n",
      "Game 73 of 100\n",
      "Game 74 of 100\n",
      "Game 75 of 100\n",
      "Game 76 of 100\n",
      "Game 77 of 100\n",
      "Game 78 of 100\n",
      "Game 79 of 100\n",
      "Game 80 of 100\n",
      "Game 81 of 100\n",
      "Game 82 of 100\n",
      "Game 83 of 100\n",
      "Game 84 of 100\n",
      "Game 85 of 100\n",
      "Game 86 of 100\n",
      "Game 87 of 100\n",
      "Game 88 of 100\n",
      "Game 89 of 100\n",
      "Game 90 of 100\n",
      "Game 91 of 100\n",
      "Game 92 of 100\n",
      "Game 93 of 100\n",
      "Game 94 of 100\n",
      "Game 95 of 100\n",
      "Game 96 of 100\n",
      "Game 97 of 100\n",
      "Game 98 of 100\n",
      "Game 99 of 100\n",
      "Game 100 of 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([15., 17., 68.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_methods(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(batch_input_shape=(1, 5)))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.add(Dense(2, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 500\n"
     ]
    }
   ],
   "source": [
    "# now execute the q learning\n",
    "num_episodes = 500\n",
    "y = 0.95\n",
    "eps = 0.5\n",
    "decay_factor = 0.999\n",
    "r_avg_list = []\n",
    "for i in range(num_episodes):\n",
    "    s = env.reset()\n",
    "    eps *= decay_factor\n",
    "    if i % 100 == 0:\n",
    "        print(\"Episode {} of {}\".format(i + 1, num_episodes))\n",
    "    done = False\n",
    "    r_sum = 0\n",
    "    while not done:\n",
    "        if np.random.random() < eps:\n",
    "            a = np.random.randint(0, 2)\n",
    "        else:\n",
    "            a = np.argmax(model.predict(np.identity(5)[s:s + 1]))\n",
    "        new_s, r, done, _ = env.step(a)\n",
    "        target = r + y * np.max(model.predict(np.identity(5)[new_s:new_s + 1]))\n",
    "        target_vec = model.predict(np.identity(5)[s:s + 1])[0]\n",
    "        target_vec[a] = target\n",
    "        model.fit(np.identity(5)[s:s + 1], target_vec.reshape(-1, 2), epochs=1, verbose=0)\n",
    "        s = new_s\n",
    "        r_sum += r\n",
    "    r_avg_list.append(r_sum / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r_avg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Tensorflow",
   "language": "python",
   "name": "python-3-tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
